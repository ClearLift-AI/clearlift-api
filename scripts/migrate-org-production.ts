#!/usr/bin/env npx tsx
/**
 * Interactive D1 Migration Wizard
 *
 * Copies org-scoped tables from production → staging/production adbliss
 * databases, interactively configures connector settings, triggers resync,
 * and provides dashboard login credentials.
 *
 * Usage:
 *   npx tsx scripts/migrate-org-production.ts              # Interactive (one org at a time)
 *   npx tsx scripts/migrate-org-production.ts --batch --staging slug1 slug2 slug3
 *   npx tsx scripts/migrate-org-production.ts --batch --production slug1 slug2
 *
 * Batch mode: migrates multiple orgs non-interactively with default settings
 * (purge+re-migrate, all_time sync, default data flow, preserve conversion config).
 *
 * The interactive wizard guides you through:
 *   1. Select environment (staging / production) — once per session
 *   2. List all orgs from PRODUCTION DB
 *   3. Select an org to migrate
 *   4. Confirm migration plan
 *   5. Copy tables (FK order, paginated)
 *   6. Interactive connector onboarding per connection
 *   7. Create session token for dashboard login
 *   8. Trigger resync + poll completion
 *   9. Ask "Migrate another org?" → loop back to step 2
 *  10. Print combined summary with login commands for all migrated orgs
 *
 * Prerequisites:
 *   1. New databases created (adbliss-core-staging / adbliss-analytics-staging)
 *   2. wrangler.jsonc updated with database IDs
 *   3. Migrations applied to new databases
 *   4. Staging API worker + queue consumer deployed
 */

import { execSync } from 'child_process';
import * as fs from 'fs';
import * as path from 'path';
import { select, confirm } from '@inquirer/prompts';
import { randomUUID } from 'crypto';

const WORK_DIR = path.resolve(__dirname, '..');
const TMP_DIR = path.join(WORK_DIR, '.migrate-prod-tmp');

// CLI flags
const args = process.argv.slice(2);
const BATCH_MODE = args.includes('--batch');
const BATCH_SLUGS = args.filter(a => !a.startsWith('--'));
const BATCH_ENV = args.includes('--staging') ? 'staging' : args.includes('--production') ? 'production' : null;

// ============================================================================
// ENV CONFIG — routes source/target DB bindings per environment
// ============================================================================

interface EnvConfig {
  label: string;
  sourceDb: string;       // Old prod DB (wrangler database-name, always prod)
  sourceAiDb: string;     // Old prod AI DB (unused for copy, kept for reference)
  targetDb: string;       // New core DB (wrangler binding name)
  targetAnalyticsDb: string;
  wranglerEnv: string;    // --env flag for TARGET wrangler commands
  apiBase: string;
  dashboardUrl: string;
}

const ENV_CONFIG: Record<string, EnvConfig> = {
  staging: {
    label: 'Staging (adbliss-core-staging)',
    sourceDb: 'clearlift-db-prod',
    sourceAiDb: 'clearlift-ai-prod',
    targetDb: 'DB',
    targetAnalyticsDb: 'ANALYTICS_DB',
    wranglerEnv: '--env staging',
    apiBase: 'https://api-dev.clearlift.ai',
    dashboardUrl: 'https://dev.clearlift.ai',
  },
  production: {
    label: 'Production (adbliss-core)',
    sourceDb: 'clearlift-db-prod',
    sourceAiDb: 'clearlift-ai-prod',
    // Uses top-level DB/ANALYTICS_DB bindings — requires wrangler.jsonc database_id
    // to already point at the new adbliss databases post-cutover
    targetDb: 'DB',
    targetAnalyticsDb: 'ANALYTICS_DB',
    wranglerEnv: '',
    apiBase: 'https://api.adbliss.io',
    dashboardUrl: 'https://app.adbliss.io',
  },
};

// Tables to copy in FK order — ONLY auth + config.
// All analytics/pipeline/AI data is rebuilt from scratch via connector resync + cron.
//
// NOT copied (by design):
//   Transient auth:    sessions, password_reset_tokens, email_verification_tokens, oauth_states
//   System-level:      waitlist, rate_limits, connector_configs (seeded), onboarding_steps (seeded)
//   Transient state:   sync_jobs, event_sync_watermarks, active_event_workflows, active_shopify_workflows
//   Admin/audit:       admin_*, audit_logs, auth_audit_logs, data_access_logs, security_events
//   AI engine:         ai_decisions, analysis_summaries, analysis_jobs, analysis_logs, analysis_events,
//                      analysis_watchlist, cac_predictions, cac_baselines (all in old AI_DB — regenerated by cron)
//   Analytics DB:      ALL tables (connector_events, conversions, ad_metrics, journeys, etc.)
//                      — rebuilt from scratch by connector resync + cron pipelines
const MIGRATION_TABLES = [
  // Phase 1: Identity (FK roots)
  'users', 'organizations', 'organization_members', 'invitations', 'platform_connections', 'org_tag_mappings',
  // Phase 2: Org config (FK → organizations)
  'ai_optimization_settings', 'dashboard_layouts', 'tracking_domains', 'script_hashes',
  'webhook_endpoints', 'org_tracking_configs', 'onboarding_progress', 'tracking_links',
  'terms_acceptance',
  // Phase 3: Connection config (FK → platform_connections)
  'connector_filter_rules',
] as const;

// Platform-specific settings keys that must be preserved from old settings
const PRESERVED_SETTINGS_KEYS: Record<string, string[]> = {
  hubspot: ['hub_id', 'hub_domain'],
  salesforce: ['instance_url'],
  shopify: ['shop_domain'],
};

// Timeframe options for connector sync
const TIMEFRAME_OPTIONS = [
  { name: 'All time (730 days) — recommended for initial migration', value: 'all_time' },
  { name: '60 days', value: '60_days' },
  { name: '7 days', value: '7_days' },
];

const TIMEFRAME_DAYS: Record<string, number> = {
  '7_days': 7,
  '60_days': 60,
  'all_time': 730,
};

let currentConfig: EnvConfig;

/** Thrown when user declines to proceed with current org — not an error, just skip. */
class OrgSkippedError extends Error {
  constructor(reason: string) { super(reason); this.name = 'OrgSkippedError'; }
}

// ============================================================================
// HELPERS
// ============================================================================

function assertSqlSafe(val: string, label: string): void {
  if (!/^[a-zA-Z0-9_\-]+$/.test(val)) {
    throw new Error(`${label} contains unsafe characters: ${val.substring(0, 50)}`);
  }
}

function log(msg: string) {
  console.log(`[${new Date().toISOString()}] ${msg}`);
}

function execOnce(cmd: string, opts?: { cwd?: string; timeout?: number }): string {
  try {
    return execSync(cmd, {
      cwd: opts?.cwd || WORK_DIR,
      encoding: 'utf-8',
      timeout: opts?.timeout || 30000,
      maxBuffer: 50 * 1024 * 1024,
      stdio: ['pipe', 'pipe', 'pipe'],
    });
  } catch (e: any) {
    const stderr = e.stderr?.toString() || '';
    const stdout = e.stdout?.toString() || '';
    if (stdout.includes('"results"') && !stderr.includes('SQLITE_ERROR')) {
      return stdout;
    }
    throw new Error(`Command failed: ${cmd}\nstderr: ${stderr}\nstdout: ${stdout}`);
  }
}

function exec(cmd: string, opts?: { cwd?: string; timeout?: number }): string {
  const MAX_RETRIES = 5;
  for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
    try {
      return execOnce(cmd, opts);
    } catch (e: any) {
      const msg = e.message || '';
      const isRateLimit = msg.includes('429') || msg.includes('rate limit') ||
        msg.includes('A request to the Clo') || msg.includes('A fetch r');
      const isSqliteBusy = msg.includes('SQLITE_BUSY') || msg.includes('database is locked');
      if ((isRateLimit || isSqliteBusy) && attempt < MAX_RETRIES) {
        const delay = isSqliteBusy ? attempt * 2 : attempt * 5;
        log(`  ${isSqliteBusy ? 'SQLITE_BUSY' : 'RATE LIMITED'} (attempt ${attempt}/${MAX_RETRIES}), waiting ${delay}s...`);
        execSync(`sleep ${delay}`);
        continue;
      }
      throw e;
    }
  }
  throw new Error('exec: unreachable');
}

function parseD1Results(output: string): any[] {
  const start = output.indexOf('[');
  const end = output.lastIndexOf(']');
  if (start === -1 || end === -1) return [];
  try {
    const json = JSON.parse(output.substring(start, end + 1));
    return json[0]?.results || [];
  } catch {
    const lines = output.split('\n');
    for (const line of lines) {
      try {
        const parsed = JSON.parse(line.trim());
        if (Array.isArray(parsed)) return parsed[0]?.results || [];
      } catch { /* skip */ }
    }
    return [];
  }
}

function escapeSQL(val: any): string {
  if (val === null || val === undefined) return 'NULL';
  if (typeof val === 'number') return String(val);
  if (typeof val === 'boolean') return val ? '1' : '0';
  const s = String(val);
  return `'${s.replace(/'/g, "''")}'`;
}

// ============================================================================
// DB ACCESS
// ============================================================================

/**
 * Query PRODUCTION source database. Always reads from prod — no --env flag.
 * Fixes Bug #1: old script used config.wranglerEnv for source reads.
 */
function querySourceProd(dbName: string, sql: string): any[] {
  const escaped = sql.replace(/"/g, '\\"');
  const output = exec(
    `npx wrangler d1 execute ${dbName} --remote --command "${escaped}" --json`,
    { timeout: 60000 }
  );
  return parseD1Results(output);
}

/**
 * Get column names from target database.
 */
function getTargetColumns(binding: string, table: string): string[] {
  const envFlag = currentConfig.wranglerEnv;
  const output = exec(
    `npx wrangler d1 execute ${binding} --remote ${envFlag} --command "PRAGMA table_info(${table})" --json`,
    { timeout: 15000 }
  );
  return parseD1Results(output).map((r: any) => r.name);
}

/**
 * Count rows in target database. Fixes Bug #3: now accepts whereClause.
 */
function countTarget(binding: string, table: string, whereClause?: string): number {
  const envFlag = currentConfig.wranglerEnv;
  const where = whereClause ? ` WHERE ${whereClause}` : '';
  const output = exec(
    `npx wrangler d1 execute ${binding} --remote ${envFlag} --command "SELECT COUNT(*) as c FROM ${table}${where}" --json`,
    { timeout: 15000 }
  );
  const results = parseD1Results(output);
  return results[0]?.c || 0;
}

/**
 * Execute a write command on the target database via temp file.
 * Uses --file instead of --command to avoid shell interpretation of
 * JSON double quotes, backticks, dollar signs, etc.
 */
function execTarget(binding: string, sql: string): void {
  const envFlag = currentConfig.wranglerEnv;
  const tmpFile = path.join(TMP_DIR, `exec_${binding}_${Date.now()}.sql`);
  if (!fs.existsSync(TMP_DIR)) fs.mkdirSync(TMP_DIR, { recursive: true });
  fs.writeFileSync(tmpFile, sql);
  try {
    exec(
      `npx wrangler d1 execute ${binding} --remote ${envFlag} --file "${tmpFile}"`,
      { timeout: 30000 }
    );
  } finally {
    try { fs.unlinkSync(tmpFile); } catch {}
  }
}

/**
 * Import a batch of rows into the target database via temp SQL file.
 */
function importBatch(binding: string, table: string, columns: string[], rows: any[]): void {
  if (rows.length === 0) return;

  const colList = columns.join(', ');
  const statements: string[] = [
    // Disable FK checks during import — source data may reference users/orgs
    // not yet copied (e.g., dashboard_layouts.updated_by → former member)
    'PRAGMA foreign_keys = OFF;',
  ];

  for (const row of rows) {
    const values = columns.map(col => escapeSQL(row[col])).join(', ');
    statements.push(`INSERT OR IGNORE INTO ${table} (${colList}) VALUES (${values});`);
  }

  statements.push('PRAGMA foreign_keys = ON;');

  const tmpFile = path.join(TMP_DIR, `import_${binding}_${table}_${Date.now()}.sql`);
  if (!fs.existsSync(TMP_DIR)) fs.mkdirSync(TMP_DIR, { recursive: true });
  fs.writeFileSync(tmpFile, statements.join('\n'));

  const envFlag = currentConfig.wranglerEnv;
  try {
    exec(
      `npx wrangler d1 execute ${binding} --remote ${envFlag} --file "${tmpFile}"`,
      { timeout: 120000 }
    );
  } finally {
    try { fs.unlinkSync(tmpFile); } catch {}
  }
}

// ============================================================================
// PAGINATED PULL: source DB (prod) → target DB (new)
// ============================================================================

function pullTable(
  sourceDbName: string,
  targetBinding: string,
  table: string,
  whereClause: string,
  label?: string
): number {
  const tag = label || table;
  const PAGE_SIZE = 500;

  // Check target table exists
  let targetCols: string[];
  try {
    targetCols = getTargetColumns(targetBinding, table);
  } catch {
    log(`  SKIP: ${tag} — table does not exist in target ${targetBinding}`);
    return 0;
  }

  // Check source table exists
  let sourceCols: any[];
  try {
    sourceCols = querySourceProd(sourceDbName, `PRAGMA table_info(${table})`);
  } catch {
    log(`  SKIP: ${tag} — table does not exist in source ${sourceDbName}`);
    return 0;
  }
  const sourceColNames = sourceCols.map((r: any) => r.name);
  if (sourceColNames.length === 0) {
    log(`  SKIP: ${tag} — table does not exist in source ${sourceDbName}`);
    return 0;
  }

  // Verify WHERE clause column exists
  const whereColMatch = whereClause.match(/^(\w+)\s*(=|IN)\s*/i);
  const whereCol = whereColMatch?.[1];
  if (whereCol && !sourceColNames.includes(whereCol)) {
    log(`  SKIP: ${tag} — source table missing column '${whereCol}'`);
    return 0;
  }

  // Count source rows
  const countResult = querySourceProd(sourceDbName, `SELECT COUNT(*) as c FROM ${table} WHERE ${whereClause}`);
  const totalRows = countResult[0]?.c || 0;
  if (totalRows === 0) {
    log(`  ${tag}: 0 rows (skipped)`);
    return 0;
  }

  log(`  ${tag}: ${totalRows.toLocaleString()} rows to copy...`);

  // Column intersection (handles schema differences between old and new)
  const commonCols = sourceColNames.filter((c: string) => targetCols.includes(c));
  if (commonCols.length === 0) {
    log(`  WARNING: No common columns between source and target for ${table}`);
    return 0;
  }

  const selectCols = commonCols.join(', ');
  let offset = 0;
  let imported = 0;

  while (offset < totalRows) {
    const rows = querySourceProd(sourceDbName, `SELECT ${selectCols} FROM ${table} WHERE ${whereClause} ORDER BY rowid LIMIT ${PAGE_SIZE} OFFSET ${offset}`);
    if (rows.length === 0) break;

    importBatch(targetBinding, table, commonCols, rows);
    imported += rows.length;
    offset += PAGE_SIZE;

    if (imported % 2000 === 0 || rows.length < PAGE_SIZE) {
      log(`    Progress: ${imported.toLocaleString()} / ${totalRows.toLocaleString()}`);
    }

    if (rows.length < PAGE_SIZE) break;
  }

  // Verify with WHERE clause (Fixes Bug #3)
  const targetCount = countTarget(targetBinding, table, whereClause);
  log(`    Verified: ${targetCount} rows in target ${table}`);
  return imported;
}

// ============================================================================
// ORG INFO
// ============================================================================

interface OrgInfo {
  id: string;
  slug: string;
  name: string;
  tag: string | null;
  memberUserIds: string[];
  primaryUserId: string;
  connections: Array<{
    id: string;
    platform: string;
    account_id: string;
    account_name: string;
    sync_status: string;
    settings: string | null;
  }>;
}

function discoverOrg(slug: string): OrgInfo {
  assertSqlSafe(slug, 'slug');

  const orgs = querySourceProd(currentConfig.sourceDb, `SELECT * FROM organizations WHERE slug = '${slug}'`);
  if (orgs.length === 0) throw new Error(`No org found with slug '${slug}'`);

  const org = orgs[0];
  const orgId = org.id;
  assertSqlSafe(orgId, 'org_id');

  // Tag mapping
  const tagRows = querySourceProd(currentConfig.sourceDb, `SELECT * FROM org_tag_mappings WHERE organization_id = '${orgId}'`);
  const tag = tagRows[0]?.short_tag || null;
  if (tag) assertSqlSafe(tag, 'org_tag');

  // Find owner or first member
  const ownerMembers = querySourceProd(currentConfig.sourceDb, `SELECT * FROM organization_members WHERE organization_id = '${orgId}' AND role = 'owner'`);
  let primaryUserId: string;
  if (ownerMembers.length > 0) {
    primaryUserId = ownerMembers[0].user_id;
  } else {
    const anyMembers = querySourceProd(currentConfig.sourceDb, `SELECT * FROM organization_members WHERE organization_id = '${orgId}' LIMIT 1`);
    if (anyMembers.length === 0) throw new Error(`No members found for org '${slug}'`);
    primaryUserId = anyMembers[0].user_id;
  }

  // All member user IDs
  const allMembers = querySourceProd(currentConfig.sourceDb, `SELECT DISTINCT user_id FROM organization_members WHERE organization_id = '${orgId}'`);
  const memberUserIds = allMembers.map((m: any) => m.user_id);
  memberUserIds.forEach((uid: string) => assertSqlSafe(uid, 'user_id'));

  // Platform connections (full row for settings)
  const connections = querySourceProd(currentConfig.sourceDb,
    `SELECT id, platform, account_id, account_name, sync_status, settings FROM platform_connections WHERE organization_id = '${orgId}'`
  );
  connections.forEach((c: any) => assertSqlSafe(c.id, 'connection_id'));

  return { id: orgId, slug: org.slug, name: org.name, tag, memberUserIds, primaryUserId, connections };
}

// ============================================================================
// STEP 1: SELECT ENVIRONMENT
// ============================================================================

async function stepSelectEnvironment(): Promise<string> {
  console.log('\n  D1 Migration Wizard\n');

  const env = await select({
    message: 'Select target environment:',
    choices: Object.entries(ENV_CONFIG).map(([key, cfg]) => ({
      name: cfg.label,
      value: key,
    })),
  });

  currentConfig = ENV_CONFIG[env];

  // Validate target DB is reachable
  log(`Validating target DB (${currentConfig.targetDb})...`);
  try {
    const envFlag = currentConfig.wranglerEnv;
    exec(
      `npx wrangler d1 execute ${currentConfig.targetDb} --remote ${envFlag} --command "SELECT 1" --json`,
      { timeout: 15000 }
    );
    log('Target DB reachable.');
  } catch (e: any) {
    console.error(`\nERROR: Cannot reach target DB '${currentConfig.targetDb}'.`);
    console.error('Make sure wrangler.jsonc has the correct database_id and migrations are applied.');
    console.error(`Detail: ${e.message?.substring(0, 200)}`);
    process.exit(1);
  }

  return env;
}

// ============================================================================
// STEP 2+3: LIST AND SELECT ORG
// ============================================================================

async function stepListAndSelectOrg(): Promise<OrgInfo> {
  log('Fetching production orgs...');

  const orgs = querySourceProd(currentConfig.sourceDb, `
    SELECT o.id, o.slug, o.name, o.subscription_tier, o.created_at
    FROM organizations o
    ORDER BY o.name
  `);

  if (orgs.length === 0) {
    console.error('No organizations found in production.');
    process.exit(1);
  }

  // Validate org IDs and build safe ID list
  for (const org of orgs) assertSqlSafe(org.id, 'org_id');

  // Batch check: which orgs already exist in target (chunked for safety)
  const migratedOrgIds = new Set<string>();
  try {
    const CHUNK_SIZE = 200;
    for (let i = 0; i < orgs.length; i += CHUNK_SIZE) {
      const chunk = orgs.slice(i, i + CHUNK_SIZE);
      const idList = chunk.map((o: any) => `'${o.id}'`).join(', ');
      const migratedRows = parseD1Results(
        exec(`npx wrangler d1 execute ${currentConfig.targetDb} --remote ${currentConfig.wranglerEnv} --command "SELECT id FROM organizations WHERE id IN (${idList})" --json`, { timeout: 15000 })
      );
      for (const row of migratedRows) migratedOrgIds.add(row.id);
    }
  } catch { /* target table may not exist yet */ }

  // Batch fetch: all connections across all orgs (chunked for safety)
  const orgConnections = new Map<string, string[]>();
  try {
    const CHUNK_SIZE = 200;
    for (let i = 0; i < orgs.length; i += CHUNK_SIZE) {
      const chunk = orgs.slice(i, i + CHUNK_SIZE);
      const idList = chunk.map((o: any) => `'${o.id}'`).join(', ');
      const connRows = querySourceProd(currentConfig.sourceDb,
        `SELECT organization_id, platform FROM platform_connections WHERE organization_id IN (${idList})`
      );
      for (const row of connRows) {
        const existing = orgConnections.get(row.organization_id) || [];
        existing.push(row.platform);
        orgConnections.set(row.organization_id, existing);
      }
    }
  } catch { /* source query failed */ }

  const selectedSlug = await select({
    message: 'Select an organization to migrate:',
    choices: orgs.map((org: any) => {
      const migrated = migratedOrgIds.has(org.id);
      const conns = orgConnections.get(org.id) || [];
      const connStr = conns.length > 0 ? ` [${conns.join(', ')}]` : '';
      const check = migrated ? ' \u2713' : '';
      return {
        name: `${org.name} (${org.slug})${connStr}${check}`,
        value: org.slug,
        description: migrated ? 'Already migrated' : undefined,
      };
    }),
  });

  log(`Discovering org: ${selectedSlug}...`);
  return discoverOrg(selectedSlug);
}

// ============================================================================
// STEP 4: CONFIRM
// ============================================================================

async function stepConfirm(info: OrgInfo): Promise<void> {
  console.log('\n--- Migration Plan ---');
  console.log(`  Org:          ${info.name} (${info.slug})`);
  console.log(`  Org ID:       ${info.id}`);
  console.log(`  Tag:          ${info.tag || '(none)'}`);
  console.log(`  Members:      ${info.memberUserIds.length} user(s)`);
  console.log(`  Connections:  ${info.connections.length > 0 ? info.connections.map(c => `${c.platform}(${c.account_name || c.account_id})`).join(', ') : '(none)'}`);
  console.log(`  Target DB:    ${currentConfig.targetDb} (${currentConfig.label})`);
  console.log(`\n  Tables to copy (in FK order):`);
  for (const table of MIGRATION_TABLES) {
    console.log(`    - ${table}`);
  }
  console.log();

  const proceed = await confirm({
    message: `Clone prod credentials for "${info.name}" into ${currentConfig.label}?`,
    default: true,
  });

  if (!proceed) {
    throw new OrgSkippedError('User declined confirmation');
  }
}

// ============================================================================
// SHARED PURGE LOGIC — used by both interactive and batch modes
// ============================================================================

function purgeOrgData(info: OrgInfo): void {
  const envFlag = currentConfig.wranglerEnv;
  const targetDb = currentConfig.targetDb;
  const targetAnalyticsDb = currentConfig.targetAnalyticsDb;
  const orgId = info.id;
  const orgWhere = `organization_id = '${orgId}'`;

  // Phase 1: Connection-level data (deepest FK deps)
  const connIds = info.connections.map(c => c.id);
  if (connIds.length > 0) {
    const connIdList = connIds.map(id => `'${id}'`).join(', ');
    for (const table of ['connector_filter_rules', 'active_shopify_workflows']) {
      log(`  Purging ${table}...`);
      try { execTarget(targetDb, `DELETE FROM ${table} WHERE connection_id IN (${connIdList})`); } catch {}
    }
  }

  // Phase 2: Pipeline state (by org_tag)
  if (info.tag) {
    for (const table of ['event_sync_watermarks', 'active_event_workflows', 'tracking_links']) {
      log(`  Purging ${table}...`);
      try {
        execTarget(targetDb, `DELETE FROM ${table} WHERE org_tag = '${info.tag}'`);
      } catch { /* table may not exist */ }
    }
  }

  // Phase 3: Org-scoped config tables (reverse of copy order)
  // ai_optimization_settings uses org_id; all others use organization_id
  const orgScopedTables: Array<{ table: string; where: string }> = [
    { table: 'org_tracking_configs', where: orgWhere },
    { table: 'webhook_endpoints', where: orgWhere },
    { table: 'script_hashes', where: orgWhere },
    { table: 'tracking_domains', where: orgWhere },
    { table: 'dashboard_layouts', where: orgWhere },
    { table: 'ai_optimization_settings', where: `org_id = '${orgId}'` },
    { table: 'onboarding_progress', where: orgWhere },
    { table: 'terms_acceptance', where: orgWhere },
    // AI engine state
    { table: 'ai_decisions', where: orgWhere },
    { table: 'analysis_summaries', where: orgWhere },
    { table: 'analysis_jobs', where: orgWhere },
    { table: 'analysis_watchlist', where: orgWhere },
    { table: 'cac_predictions', where: orgWhere },
    { table: 'cac_baselines', where: orgWhere },
  ];

  for (const { table, where } of orgScopedTables) {
    log(`  Purging ${table}...`);
    try { execTarget(targetDb, `DELETE FROM ${table} WHERE ${where}`); } catch {}
  }

  // Phase 4: Org tag mappings, platform connections, invitations
  for (const table of ['org_tag_mappings', 'platform_connections', 'invitations']) {
    log(`  Purging ${table}...`);
    try { execTarget(targetDb, `DELETE FROM ${table} WHERE ${orgWhere}`); } catch {}
  }

  // Phase 5: Organization members
  log('  Purging organization_members...');
  try { execTarget(targetDb, `DELETE FROM organization_members WHERE ${orgWhere}`); } catch {}

  // Phase 6: Analytics DB — ALL org-scoped tables with explicit column mapping.
  // Uses deterministic col→value mapping instead of try/catch fallback, because
  // DELETE with wrong column succeeds with 0 rows (doesn't throw) when the table
  // has both organization_id and org_tag columns.
  {
    const orgIdWhere = `organization_id = '${orgId}'`;
    const orgTagWhere = info.tag ? `org_tag = '${info.tag}'` : null;

    const analyticsOrgTables: Array<{ table: string; where: string }> = [
      // organization_id-scoped tables
      { table: 'identity_mappings', where: orgIdWhere },
      { table: 'identity_merges', where: orgIdWhere },
      { table: 'connector_events', where: orgIdWhere },
      { table: 'connector_customers', where: orgIdWhere },
      { table: 'connector_activities', where: orgIdWhere },
      { table: 'conversions', where: orgIdWhere },
      { table: 'conversion_attribution', where: orgIdWhere },
      { table: 'conversion_daily_summary', where: orgIdWhere },
      { table: 'conversion_value_allocations', where: orgIdWhere },
      { table: 'ad_campaigns', where: orgIdWhere },
      { table: 'ad_groups', where: orgIdWhere },
      { table: 'ads', where: orgIdWhere },
      { table: 'ad_metrics', where: orgIdWhere },
      { table: 'facebook_pages', where: orgIdWhere },
      { table: 'identity_link_events', where: orgIdWhere },
      { table: 'webhook_events', where: orgIdWhere },
      { table: 'cac_history', where: orgIdWhere },
      { table: 'domain_claims', where: orgIdWhere },
      { table: 'tracked_clicks', where: orgIdWhere },
      { table: 'connector_sync_status', where: orgIdWhere },
      { table: 'handoff_patterns', where: orgIdWhere },
      { table: 'handoff_observations', where: orgIdWhere },
      // org_tag-scoped tables (only purge if tag exists)
      ...(orgTagWhere ? [
        { table: 'customer_identities', where: orgTagWhere },
        { table: 'touchpoints', where: orgTagWhere },
        { table: 'attribution_results', where: orgTagWhere },
        { table: 'journeys', where: orgTagWhere },
        { table: 'journey_analytics', where: orgTagWhere },
        { table: 'channel_transitions', where: orgTagWhere },
        { table: 'funnel_transitions', where: orgTagWhere },
        { table: 'hourly_metrics', where: orgTagWhere },
        { table: 'daily_metrics', where: orgTagWhere },
        { table: 'utm_performance', where: orgTagWhere },
        { table: 'sync_watermarks', where: orgTagWhere },
      ] : []),
    ];

    for (const { table, where } of analyticsOrgTables) {
      log(`  Purging ${table} (analytics)...`);
      try { execTarget(targetAnalyticsDb, `DELETE FROM ${table} WHERE ${where}`); } catch {}
    }
  }

  // Phase 7: Sessions for all member users
  for (const uid of info.memberUserIds) {
    log(`  Purging sessions for user ${uid.substring(0, 8)}...`);
    try { execTarget(targetDb, `DELETE FROM sessions WHERE user_id = '${uid}'`); } catch {}
  }

  // Phase 8: Organization itself
  log('  Purging organization...');
  try { execTarget(targetDb, `DELETE FROM organizations WHERE id = '${orgId}'`); } catch {}

  // Phase 9: Users (only if they have no other org memberships in target)
  for (const uid of info.memberUserIds) {
    try {
      const otherMemberships = parseD1Results(
        exec(`npx wrangler d1 execute ${targetDb} --remote ${envFlag} --command "SELECT COUNT(*) as c FROM organization_members WHERE user_id = '${uid}'" --json`, { timeout: 15000 })
      );
      if ((otherMemberships[0]?.c || 0) === 0) {
        log(`  Purging user ${uid.substring(0, 8)}... (no other org memberships)`);
        execTarget(targetDb, `DELETE FROM users WHERE id = '${uid}'`);
      } else {
        log(`  Keeping user ${uid.substring(0, 8)}... (member of other orgs)`);
      }
    } catch {}
  }
}

// ============================================================================
// STEP 4b: PURGE EXISTING ORG (if already migrated)
// ============================================================================

async function stepPurgeIfExists(info: OrgInfo): Promise<void> {
  const envFlag = currentConfig.wranglerEnv;
  const targetDb = currentConfig.targetDb;
  const targetAnalyticsDb = currentConfig.targetAnalyticsDb;

  // Check if org already exists in target
  let orgExists = false;
  try {
    const result = parseD1Results(
      exec(`npx wrangler d1 execute ${targetDb} --remote ${envFlag} --command "SELECT id FROM organizations WHERE id = '${info.id}'" --json`, { timeout: 15000 })
    );
    orgExists = result.length > 0;
  } catch { /* table may not exist */ }

  if (!orgExists) return;

  console.log(`\n  ⚠  Org "${info.name}" already exists in ${currentConfig.label}.`);

  const action = await select({
    message: 'How do you want to handle the existing data?',
    choices: [
      { name: 'Purge and re-migrate (clean slate — deletes all org data from target, then copies fresh from prod)', value: 'purge' },
      { name: 'Skip copy, just resync (keep existing data, skip to connector onboarding + resync)', value: 'skip' },
      { name: 'Overlay (INSERT OR IGNORE — add missing rows, keep existing)', value: 'overlay' },
      { name: 'Abort', value: 'abort' },
    ],
  });

  if (action === 'abort') {
    throw new OrgSkippedError('User aborted purge decision');
  }

  if (action === 'skip') {
    // Set a flag so main() skips stepCopy (onboarding still runs)
    (info as any)._skipCopy = true;
    return;
  }

  if (action === 'overlay') {
    // Default behavior — INSERT OR IGNORE handles it
    return;
  }

  // === PURGE ===
  const confirmPurge = await confirm({
    message: `DELETE all data for "${info.name}" from ${currentConfig.label}? This cannot be undone.`,
    default: false,
  });

  if (!confirmPurge) {
    throw new OrgSkippedError('User declined purge confirmation');
  }

  log('\nPurging existing org data (reverse FK order)...');
  purgeOrgData(info);
  log('\nPurge complete. Proceeding with fresh copy...');
}

// ============================================================================
// STEP 5: COPY TABLES
// ============================================================================

function stepCopy(info: OrgInfo): void {
  log('\nCopying core tables (FK order)...');

  const orgWhere = `organization_id = '${info.id}'`;
  const targetDb = currentConfig.targetDb;
  let totalRows = 0;
  let tablesCopied = 0;

  function track(n: number) { totalRows += n; if (n > 0) tablesCopied++; }

  // ---- Phase 1: Identity tables (FK roots) ----

  // 1. Users (all members — avoids FK violations)
  for (const uid of info.memberUserIds) {
    track(pullTable(currentConfig.sourceDb, targetDb, 'users', `id = '${uid}'`, `users (${uid.substring(0, 8)}...)`));
  }

  // 2. Organizations
  track(pullTable(currentConfig.sourceDb, targetDb, 'organizations', `id = '${info.id}'`));

  // 3. Organization members
  track(pullTable(currentConfig.sourceDb, targetDb, 'organization_members', orgWhere));

  // 4. Invitations (pending team invites)
  track(pullTable(currentConfig.sourceDb, targetDb, 'invitations', orgWhere));

  // 5. Platform connections (encrypted OAuth tokens — irreplaceable)
  track(pullTable(currentConfig.sourceDb, targetDb, 'platform_connections', orgWhere));

  // 6. Org tag mappings
  track(pullTable(currentConfig.sourceDb, targetDb, 'org_tag_mappings', orgWhere));

  // ---- Phase 2: Org-level config (FK → organizations) ----

  track(pullTable(currentConfig.sourceDb, targetDb, 'ai_optimization_settings', `org_id = '${info.id}'`, 'ai_optimization_settings'));
  track(pullTable(currentConfig.sourceDb, targetDb, 'dashboard_layouts', orgWhere));
  track(pullTable(currentConfig.sourceDb, targetDb, 'tracking_domains', orgWhere));
  track(pullTable(currentConfig.sourceDb, targetDb, 'script_hashes', orgWhere));
  track(pullTable(currentConfig.sourceDb, targetDb, 'webhook_endpoints', orgWhere));
  track(pullTable(currentConfig.sourceDb, targetDb, 'org_tracking_configs', orgWhere));
  track(pullTable(currentConfig.sourceDb, targetDb, 'onboarding_progress', orgWhere));

  // Tracking links use org_tag, not organization_id
  if (info.tag) {
    track(pullTable(currentConfig.sourceDb, targetDb, 'tracking_links', `org_tag = '${info.tag}'`, 'tracking_links (by org_tag)'));
  }

  // Terms acceptance
  track(pullTable(currentConfig.sourceDb, targetDb, 'terms_acceptance', orgWhere));

  // ---- Phase 3: Connection-level config (FK → platform_connections) ----

  const connectionIds = info.connections.map(c => c.id);
  if (connectionIds.length > 0) {
    const connIdList = connectionIds.map(id => `'${id}'`).join(', ');
    track(pullTable(currentConfig.sourceDb, targetDb, 'connector_filter_rules', `connection_id IN (${connIdList})`, 'connector_filter_rules'));
  }

  // NOT copied: AI engine tables (old AI_DB), pipeline state (transient), analytics (rebuilt by resync)

  log(`\nCopy complete: ${totalRows} total rows across ${tablesCopied} tables.`);
}

// ============================================================================
// STEP 6: INTERACTIVE CONNECTOR ONBOARDING
// ============================================================================

async function stepOnboardConnectors(info: OrgInfo): Promise<void> {
  if (info.connections.length === 0) {
    log('\nNo connectors to configure.');
    return;
  }

  console.log(`\n--- Connector Onboarding (${info.connections.length} connection(s)) ---\n`);

  for (const conn of info.connections) {
    console.log(`\n  ${conn.platform.toUpperCase()} — ${conn.account_name || conn.account_id} (${conn.id.substring(0, 8)}...)`);

    // Parse old settings to preserve platform-specific keys
    const oldSettings = conn.settings ? JSON.parse(conn.settings) : {};

    // 6a: Sync timeframe
    const timeframe = await select({
      message: `  Sync window for ${conn.platform}?`,
      choices: TIMEFRAME_OPTIONS,
      default: 'all_time',
    });

    // Build new settings
    const newSettings: Record<string, any> = {
      sync_config: { timeframe },
      emit_events: true,
      aggregation_mode: 'conversions_only',
      dedup_window_hours: 24,
    };

    // 6b: Google Ads — account selection
    if (conn.platform === 'google') {
      const oldAccountMode = oldSettings.accountSelection?.mode || 'unknown';
      const oldSelected = oldSettings.accountSelection?.selectedAccounts || [];

      const accountMode = await select({
        message: `  Google Ads account mode? (current: ${oldAccountMode}, ${oldSelected.length} selected)`,
        choices: [
          { name: 'All accounts (recommended)', value: 'all' },
          { name: 'Keep existing selection', value: 'keep' },
        ],
        default: 'all',
      });

      if (accountMode === 'all') {
        newSettings.accountSelection = { mode: 'all' };
      } else {
        // Preserve existing selection
        newSettings.accountSelection = oldSettings.accountSelection || { mode: 'all' };
      }
    }

    // 6c: Data flow defaults
    const useDefaults = await confirm({
      message: `  Use default data flow? (emit_events=true, aggregation_mode=conversions_only, dedup_window=24h)`,
      default: true,
    });

    if (!useDefaults) {
      const emitEvents = await confirm({ message: '    emit_events?', default: true });
      newSettings.emit_events = emitEvents;

      const aggMode = await select({
        message: '    aggregation_mode?',
        choices: [
          { name: 'conversions_only', value: 'conversions_only' },
          { name: 'all_events', value: 'all_events' },
          { name: 'none', value: 'none' },
        ],
        default: 'conversions_only',
      });
      newSettings.aggregation_mode = aggMode;
    }

    // 6d: Revenue connectors — platform-specific config
    if (conn.platform === 'stripe') {
      const syncMode = await select({
        message: '  Stripe sync mode?',
        choices: [
          { name: 'Charges (one-time payments)', value: 'charges' },
          { name: 'Subscriptions (recurring)', value: 'subscriptions' },
        ],
        default: oldSettings.sync_mode || 'charges',
      });
      newSettings.sync_mode = syncMode;
      newSettings.lookback_days = TIMEFRAME_DAYS[timeframe] || 60;
      newSettings.auto_sync = true;
      // Remove non-Stripe keys
      delete newSettings.aggregation_mode;
      delete newSettings.dedup_window_hours;
      delete newSettings.emit_events;
    }

    // 6d2: Preserve conversion config for ALL connector types
    // conversion_events, conversion_pages, and pre_conversion_pages drive the
    // entire conversion + attribution pipeline. Losing them silently breaks CAC.
    const CONVERSION_CONFIG_KEYS = ['conversion_events', 'conversion_pages', 'pre_conversion_pages'] as const;
    const hasAnyConvConfig = CONVERSION_CONFIG_KEYS.some(k => oldSettings[k]);
    if (hasAnyConvConfig) {
      const convConfigSummary = CONVERSION_CONFIG_KEYS
        .filter(k => oldSettings[k])
        .map(k => `${k}: ${JSON.stringify(oldSettings[k])}`)
        .join('\n      ');
      console.log(`    Current conversion config:\n      ${convConfigSummary}`);
      const keepConvConfig = await confirm({
        message: '    Keep existing conversion config (conversion_events, conversion_pages, pre_conversion_pages)?',
        default: true,
      });
      if (keepConvConfig) {
        for (const key of CONVERSION_CONFIG_KEYS) {
          if (oldSettings[key] !== undefined) {
            newSettings[key] = oldSettings[key];
          }
        }
      }
    }

    // 6e: MERGE old settings (preserve platform-specific keys) then WRITE
    const preservedKeys = PRESERVED_SETTINGS_KEYS[conn.platform] || [];
    const preserved: Record<string, any> = {};
    for (const key of preservedKeys) {
      if (oldSettings[key] !== undefined) {
        preserved[key] = oldSettings[key];
      }
    }

    const mergedSettings = { ...preserved, ...newSettings };
    // Safe: JSON.stringify produces only double-quoted strings, so single-quote
    // escaping is sufficient. No user-controlled input enters the JSON keys.
    const settingsJson = JSON.stringify(mergedSettings).replace(/'/g, "''");

    log(`  Writing settings for ${conn.platform} (${conn.id.substring(0, 8)}...)...`);
    execTarget(
      currentConfig.targetDb,
      `UPDATE platform_connections SET settings = '${settingsJson}' WHERE id = '${conn.id}'`
    );
    log(`  Settings saved.`);
  }
}

// ============================================================================
// STEP 7: CREATE SESSION TOKEN
// ============================================================================

function stepCreateSession(info: OrgInfo): string {
  log('\nCreating session token...');

  const token = randomUUID();

  execTarget(
    currentConfig.targetDb,
    `INSERT INTO sessions (token, user_id, created_at, expires_at, user_agent) VALUES ('${token}', '${info.primaryUserId}', datetime('now'), datetime('now', '+30 days'), 'migrate-org-wizard')`
  );

  log(`Session created for user ${info.primaryUserId.substring(0, 8)}... (expires in 30 days)`);
  return token;
}

// ============================================================================
// TRIGGER DOWNSTREAM PIPELINES (shared by interactive + batch modes)
// ============================================================================

/**
 * After connector resyncs complete, trigger the full pipeline cascade:
 *   1. Conversion aggregation → linking → CAC refresh (via resync-all endpoint)
 *   2. Recalculation pipeline (aggregation cascade for conversion data)
 *   3. CAC backfill (historical CAC data)
 *
 * The hourly cron (once deployed with schedule) will also trigger:
 *   - HourlyAnalyticsWorkflow (chains: AI recs → CAC refresh → probabilistic attribution)
 *   - Identity extraction + conversion linking (daily)
 *
 * For immediate results, we explicitly trigger the recalculation and CAC endpoints.
 */
async function triggerDownstreamPipelines(info: OrgInfo, headers: Record<string, string>): Promise<void> {
  log('\nTriggering downstream pipelines...');

  // 1. Recalculation cascade: conversion aggregation → linking → CAC refresh
  try {
    const resp = await fetch(`${currentConfig.apiBase}/v1/workers/recalculate/trigger?org_id=${info.id}`, {
      method: 'POST',
      headers,
      body: JSON.stringify({ days: 90 }),
    });
    if (resp.ok) {
      log('  Recalculation pipeline queued (aggregation → linking → CAC).');
    } else {
      const body = await resp.text();
      log(`  Recalculation failed (${resp.status}): ${body.substring(0, 100)}`);
    }
  } catch (e: any) {
    log(`  Recalculation error: ${e.message?.substring(0, 80)}`);
  }

  // 2. CAC backfill (historical daily CAC)
  try {
    const resp = await fetch(`${currentConfig.apiBase}/v1/analytics/cac/backfill?org_id=${info.id}`, {
      method: 'POST',
      headers,
      body: JSON.stringify({ org_id: info.id, days: 90 }),
    });
    if (resp.ok) {
      log('  CAC backfill triggered (90 days).');
    } else {
      const body = await resp.text();
      log(`  CAC backfill failed (${resp.status}): ${body.substring(0, 100)}`);
    }
  } catch (e: any) {
    log(`  CAC backfill error: ${e.message?.substring(0, 80)}`);
  }

  // 3. Trigger resync-all (platform syncs + aggregation cascade, if not already triggered above)
  //    This endpoint also handles skip_platform_sync mode for just the cascade
  try {
    const resp = await fetch(`${currentConfig.apiBase}/v1/workers/resync-all/trigger?org_id=${info.id}`, {
      method: 'POST',
      headers,
      body: JSON.stringify({ days: 30, skip_platform_sync: true }),
    });
    if (resp.ok) {
      log('  Full pipeline cascade queued (resync-all).');
    } else if ((resp.status as number) === 429) {
      log('  Resync-all already in progress (429 — expected if resyncs were just triggered).');
    } else {
      const body = await resp.text();
      log(`  Resync-all failed (${resp.status}): ${body.substring(0, 100)}`);
    }
  } catch (e: any) {
    log(`  Resync-all error: ${e.message?.substring(0, 80)}`);
  }

  log('  Downstream pipelines triggered. Probabilistic attribution + page flow');
  log('  will run on the next hourly cron cycle (*/1 * * * * on staging).');
}

// ============================================================================
// STEP 8: TRIGGER RESYNC
// ============================================================================

async function stepTriggerSync(info: OrgInfo, sessionToken: string): Promise<void> {
  if (info.connections.length === 0) {
    log('\nNo connectors to sync.');
    return;
  }

  const doResync = await confirm({
    message: `Trigger resync for ${info.connections.length} connector(s)?`,
    default: true,
  });

  if (!doResync) {
    log('Skipping resync. You can trigger manually from the dashboard.');
    return;
  }

  // Health check
  log(`\nChecking API health at ${currentConfig.apiBase}...`);
  try {
    const healthResp = await fetch(`${currentConfig.apiBase}/v1/health`);
    if (!healthResp.ok) {
      log(`WARNING: API health check failed (${healthResp.status}). Continuing anyway...`);
    } else {
      log('API is healthy.');
    }
  } catch (e: any) {
    log(`WARNING: Cannot reach API at ${currentConfig.apiBase}: ${e.message?.substring(0, 80)}`);
    const proceed = await confirm({ message: 'Continue with resync anyway?', default: false });
    if (!proceed) return;
  }

  const headers = {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${sessionToken}`,
    'X-Organization-Id': info.id,
  };

  // Trigger resync for each connection
  const syncResults: Array<{ connectionId: string; platform: string; status: string; jobId?: string }> = [];

  for (const conn of info.connections) {
    try {
      const resp = await fetch(`${currentConfig.apiBase}/v1/connectors/${conn.id}/resync`, {
        method: 'POST',
        headers,
      });

      const body = await resp.json() as any;

      if (resp.ok) {
        const jobId = body.data?.job_id || body.job_id;
        syncResults.push({ connectionId: conn.id, platform: conn.platform, status: 'triggered', jobId });
        log(`  Triggered: ${conn.platform} -> job ${jobId?.substring(0, 8)}...`);
      } else if (resp.status === 409) {
        syncResults.push({ connectionId: conn.id, platform: conn.platform, status: 'already_syncing' });
        log(`  Already syncing: ${conn.platform} (409)`);
      } else {
        syncResults.push({ connectionId: conn.id, platform: conn.platform, status: `error_${resp.status}` });
        log(`  ERROR: ${conn.platform} (${resp.status}): ${JSON.stringify(body).substring(0, 100)}`);
      }
    } catch (e: any) {
      syncResults.push({ connectionId: conn.id, platform: conn.platform, status: 'network_error' });
      log(`  ERROR: ${conn.platform} network error: ${e.message?.substring(0, 80)}`);
    }
  }

  // Poll sync completion
  const activeSyncs = syncResults.filter(r => r.status === 'triggered' && r.jobId);
  if (activeSyncs.length > 0) {
    const doPoll = await confirm({
      message: `Poll ${activeSyncs.length} sync(s) for completion? (can take 5-30 min)`,
      default: true,
    });

    if (doPoll) {
      log(`\nPolling ${activeSyncs.length} sync(s)...`);

      const POLL_INTERVAL_MS = 30_000;
      const POLL_TIMEOUT_MS = 30 * 60 * 1000;
      const startTime = Date.now();
      const pending = new Map(activeSyncs.map(s => [s.connectionId, s]));

      while (pending.size > 0 && (Date.now() - startTime) < POLL_TIMEOUT_MS) {
        await new Promise(resolve => setTimeout(resolve, POLL_INTERVAL_MS));

        for (const [connId, sync] of [...pending.entries()]) {
          try {
            const resp = await fetch(`${currentConfig.apiBase}/v1/connectors/${connId}/sync-status`, { headers });
            const body = await resp.json() as any;
            const status = body.latest_sync?.status || body.connection?.sync_status;

            if (status === 'completed') {
              log(`  ${sync.platform} sync completed`);
              pending.delete(connId);
            } else if (status === 'failed') {
              log(`  ${sync.platform} sync FAILED: ${body.latest_sync?.error || 'unknown'}`);
              pending.delete(connId);
            } else {
              const progress = body.latest_sync?.progress_percentage || '?';
              log(`  ${sync.platform}: ${status} (${progress}%)`);
            }
          } catch { /* keep waiting */ }
        }
      }

      if (pending.size > 0) {
        log(`WARNING: ${pending.size} sync(s) still running after 30 min. They will continue in background.`);
      }
    }
  }

  // Trigger downstream pipelines (aggregation → linking → CAC, attribution, identity)
  await triggerDownstreamPipelines(info, headers);

  // Summary
  console.log('\n  Sync summary:');
  for (const r of syncResults) {
    const symbol = r.status === 'triggered' ? '\u2713' : r.status === 'already_syncing' ? '~' : '\u2717';
    console.log(`    ${symbol} ${r.platform}: ${r.status}`);
  }
}

// stepComplete removed — replaced by printFinalSummary in multi-org main loop

// ============================================================================
// MAIN — supports multiple orgs in a single session
// ============================================================================

interface MigrationResult {
  orgName: string;
  orgSlug: string;
  orgId: string;
  sessionToken?: string;
  success: boolean;
  error?: string;
}

async function migrateOneOrg(): Promise<MigrationResult> {
  // Step 2+3: List orgs, select one (re-fetches to show updated ✓ marks)
  const orgInfo = await stepListAndSelectOrg();

  try {
    // Step 4: Confirm
    await stepConfirm(orgInfo);

    // Step 4b: Purge if org already exists in target
    await stepPurgeIfExists(orgInfo);

    if (!(orgInfo as any)._skipCopy) {
      // Step 5: Copy tables
      stepCopy(orgInfo);
    }

    // Step 6: Interactive connector onboarding (runs even for "skip copy" —
    // user may want to reconfigure settings without re-copied tables)
    await stepOnboardConnectors(orgInfo);

    // Step 7: Create session token
    const sessionToken = stepCreateSession(orgInfo);

    // Step 8: Trigger resync
    await stepTriggerSync(orgInfo, sessionToken);

    // Brief per-org confirmation
    console.log(`\n  ✓ ${orgInfo.name} (${orgInfo.slug}) migrated successfully.`);

    return {
      orgName: orgInfo.name,
      orgSlug: orgInfo.slug,
      orgId: orgInfo.id,
      sessionToken,
      success: true,
    };
  } catch (e) {
    // Re-throw with orgInfo attached so the caller can track the failure
    (e as any)._orgInfo = orgInfo;
    throw e;
  }
}

function printFinalSummary(results: MigrationResult[]): void {
  console.log('\n' + '='.repeat(60));
  console.log('  Migration Session Complete!');
  console.log('='.repeat(60));
  const succeeded = results.filter(r => r.success).length;
  const failed = results.length - succeeded;
  const summary = failed > 0 ? `${succeeded} succeeded, ${failed} failed` : `${succeeded} org(s) migrated`;
  console.log(`\n  ${summary}:`);

  for (const r of results) {
    const icon = r.success ? '✓' : '✗';
    console.log(`    ${icon} ${r.orgName} (${r.orgSlug})${r.error ? ` — ${r.error}` : ''}`);
  }

  console.log(`\n  Dashboard: ${currentConfig.dashboardUrl}`);
  console.log('\n  Paste in Chrome DevTools console to login as each org:\n');

  for (const r of results) {
    if (!r.success) continue;
    console.log(`  // --- ${r.orgName} (${r.orgSlug}) ---`);
    console.log(`  localStorage.setItem('adbliss_session', '${r.sessionToken}');`);
    console.log(`  localStorage.setItem('adbliss_current_org', '${r.orgId}');`);
    console.log(`  location.reload();\n`);
  }

  console.log('  Sessions expire in 30 days. Treat these tokens as credentials.');
  console.log('  Downstream workflows (identity, attribution) will fire on next cron cycle.');
  console.log();
}

// ============================================================================
// BATCH MODE — non-interactive migration of multiple orgs
// ============================================================================

function batchOnboardConnectors(info: OrgInfo): void {
  // Non-interactive: apply default settings to all connectors
  // Preserves conversion config, platform-specific keys, and uses all_time sync
  for (const conn of info.connections) {
    const oldSettings = conn.settings ? JSON.parse(conn.settings) : {};

    const newSettings: Record<string, any> = {
      sync_config: { timeframe: 'all_time' },
      emit_events: true,
      aggregation_mode: 'conversions_only',
      dedup_window_hours: 24,
    };

    // Stripe: preserve sync_mode or default to charges
    if (conn.platform === 'stripe') {
      newSettings.sync_mode = oldSettings.sync_mode || 'charges';
      newSettings.lookback_days = 730;
      newSettings.auto_sync = true;
      delete newSettings.aggregation_mode;
      delete newSettings.dedup_window_hours;
      delete newSettings.emit_events;
    }

    // Google: preserve account selection
    if (conn.platform === 'google' && oldSettings.accountSelection) {
      newSettings.accountSelection = oldSettings.accountSelection;
    }

    // Preserve conversion config (critical for pipeline)
    for (const key of ['conversion_events', 'conversion_pages', 'pre_conversion_pages'] as const) {
      if (oldSettings[key] !== undefined) {
        newSettings[key] = oldSettings[key];
      }
    }

    // Preserve platform-specific keys
    const preservedKeys = PRESERVED_SETTINGS_KEYS[conn.platform] || [];
    for (const key of preservedKeys) {
      if (oldSettings[key] !== undefined) {
        newSettings[key] = oldSettings[key];
      }
    }

    const settingsJson = JSON.stringify(newSettings).replace(/'/g, "''");
    log(`  [batch] Writing settings for ${conn.platform} (${conn.id.substring(0, 8)}...)...`);
    execTarget(
      currentConfig.targetDb,
      `UPDATE platform_connections SET settings = '${settingsJson}' WHERE id = '${conn.id}'`
    );
  }
}

async function batchMigrateOrg(slug: string): Promise<MigrationResult> {
  log(`\n${'─'.repeat(50)}`);
  log(`Batch migrating: ${slug}`);

  const info = discoverOrg(slug);
  log(`  Org: ${info.name} (${info.id})`);
  log(`  Tag: ${info.tag || '(none)'}`);
  log(`  Members: ${info.memberUserIds.length}, Connections: ${info.connections.length}`);

  try {
    // Purge if exists (non-interactive: always purge + re-migrate)
    const envFlag = currentConfig.wranglerEnv;
    let orgExists = false;
    try {
      const result = parseD1Results(
        exec(`npx wrangler d1 execute ${currentConfig.targetDb} --remote ${envFlag} --command "SELECT id FROM organizations WHERE id = '${info.id}'" --json`, { timeout: 15000 })
      );
      orgExists = result.length > 0;
    } catch {}

    if (orgExists) {
      log(`  Org exists in target — purging for clean re-migrate...`);
      purgeOrgData(info);
      log(`  Purge complete.`);
    }

    // Copy tables
    stepCopy(info);

    // Non-interactive connector onboarding (default settings)
    batchOnboardConnectors(info);

    // Create session token
    const sessionToken = stepCreateSession(info);

    // Trigger resync (non-interactive: always trigger, no polling)
    if (info.connections.length > 0) {
      log(`\nTriggering resync for ${info.connections.length} connector(s)...`);
      const headers = {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${sessionToken}`,
        'X-Organization-Id': info.id,
      };

      for (const conn of info.connections) {
        try {
          const resp = await fetch(`${currentConfig.apiBase}/v1/connectors/${conn.id}/resync`, {
            method: 'POST',
            headers,
          });
          if (resp.ok) {
            log(`  Triggered: ${conn.platform}`);
          } else if (resp.status === 409) {
            log(`  Already syncing: ${conn.platform}`);
          } else {
            log(`  ERROR: ${conn.platform} (${resp.status})`);
          }
        } catch (e: any) {
          log(`  ERROR: ${conn.platform}: ${e.message?.substring(0, 60)}`);
        }
      }

      // Trigger downstream pipelines (aggregation → linking → CAC, attribution)
      await triggerDownstreamPipelines(info, headers);
    }

    log(`  Done: ${info.name}`);
    return { orgName: info.name, orgSlug: info.slug, orgId: info.id, sessionToken, success: true };
  } catch (e: any) {
    const msg = e.message || String(e);
    log(`  FAILED: ${info.name} — ${msg.substring(0, 120)}`);
    return { orgName: info.name, orgSlug: info.slug, orgId: info.id, success: false, error: msg.substring(0, 120) };
  }
}

async function main() {
  if (!fs.existsSync(TMP_DIR)) fs.mkdirSync(TMP_DIR, { recursive: true });

  try {
    // ── BATCH MODE ──────────────────────────────────────────────────────
    if (BATCH_MODE) {
      if (!BATCH_ENV) {
        console.error('Batch mode requires --staging or --production flag.');
        console.error('Usage: npx tsx scripts/migrate-org-production.ts --batch --staging slug1 slug2 ...');
        process.exit(1);
      }
      if (BATCH_SLUGS.length === 0) {
        console.error('Batch mode requires at least one org slug.');
        process.exit(1);
      }

      currentConfig = ENV_CONFIG[BATCH_ENV];
      console.log(`\n  Batch Migration — ${currentConfig.label}`);
      console.log(`  Orgs: ${BATCH_SLUGS.join(', ')}\n`);

      // Validate target DB
      try {
        exec(
          `npx wrangler d1 execute ${currentConfig.targetDb} --remote ${currentConfig.wranglerEnv} --command "SELECT 1" --json`,
          { timeout: 15000 }
        );
      } catch (e: any) {
        console.error(`Cannot reach target DB. Detail: ${e.message?.substring(0, 200)}`);
        process.exit(1);
      }

      const results: MigrationResult[] = [];
      for (const slug of BATCH_SLUGS) {
        try {
          const result = await batchMigrateOrg(slug);
          results.push(result);
        } catch (e: any) {
          results.push({ orgName: slug, orgSlug: slug, orgId: '?', success: false, error: (e.message || String(e)).substring(0, 120) });
        }
      }

      printFinalSummary(results);
      return;
    }

    // ── INTERACTIVE MODE ────────────────────────────────────────────────
    // Step 1: Select environment (once per session)
    await stepSelectEnvironment();

    const results: MigrationResult[] = [];

    // Loop: migrate orgs until user is done
    let keepGoing = true;
    while (keepGoing) {
      try {
        const result = await migrateOneOrg();
        results.push(result);
      } catch (e: any) {
        if (e instanceof OrgSkippedError) {
          console.log(`\n  Skipped: ${e.message}`);
        } else {
          const msg = e.message || String(e);
          console.error(`\n  Migration failed: ${msg}`);
          console.error('  If the failure occurred during table copy, the migration is');
          console.error('  partially complete. You can retry — INSERT OR IGNORE skips existing rows.\n');
          // Track the failure so it appears in the final summary
          const orgInfo = e._orgInfo as OrgInfo | undefined;
          if (orgInfo) {
            results.push({
              orgName: orgInfo.name,
              orgSlug: orgInfo.slug,
              orgId: orgInfo.id,
              success: false,
              error: msg.substring(0, 120),
            });
          }
        }
        // Don't exit — let user continue with other orgs
      }

      keepGoing = await confirm({
        message: 'Migrate another org?',
        default: !results.some(r => r.success), // Default yes if none succeeded yet
      });
    }

    // Final summary with all login commands
    if (results.length > 0) {
      printFinalSummary(results);
    } else {
      console.log('\nNo orgs migrated.');
    }
  } finally {
    try { fs.rmSync(TMP_DIR, { recursive: true, force: true }); } catch {}
  }
}

main().catch(e => {
  console.error('\nMigration session failed:', e.message || e);
  try { fs.rmSync(TMP_DIR, { recursive: true, force: true }); } catch {}
  process.exit(1);
});
